{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./final_dataset (1).csv\",encoding='latin-1')\n",
    "#cols=['Tweet Id', 'Tweets','User Id', 'Screen Name', 'Class']\n",
    "df=df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets=df[\"Tweets\"].size\n",
    "def tweet_processing(raw_tweet):\n",
    "    letters_only=re.sub(\"[^a-zA-Z]\",\" \",raw_tweet)\n",
    "    words=letters_only.lower().split()\n",
    "    stops=set(stopwords.words(\"english\"))\n",
    "    m_w=[w for w in words if not w in stops]\n",
    "    return (\" \".join(m_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet=[]\n",
    "for i in range(0,num_tweets):\n",
    "    clean_tweet.append(tweet_processing(df[\"Tweets\"][i]))\n",
    "\n",
    "df[\"Tweets\"]=clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User Id</th>\n",
       "      <th>Screen Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.190000e+17</td>\n",
       "      <td>oh yeah colin smash girls mkr</td>\n",
       "      <td>949380854</td>\n",
       "      <td>CreatrixKali</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.210000e+17</td>\n",
       "      <td>insane keep bringing people back show end mkr</td>\n",
       "      <td>297877558</td>\n",
       "      <td>quincepoacher</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.240000e+17</td>\n",
       "      <td>berkeley eagle mkr shit show comebacks johnny ...</td>\n",
       "      <td>272704749</td>\n",
       "      <td>MarkTramby</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.260000e+17</td>\n",
       "      <td>sigh oh colin mkr</td>\n",
       "      <td>71416635</td>\n",
       "      <td>BinnyD</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.820000e+17</td>\n",
       "      <td>rt brian day swear sexist honestly cannot stan...</td>\n",
       "      <td>930620467</td>\n",
       "      <td>YesYoureSexist</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Tweets    User Id  \\\n",
       "0  3.190000e+17                      oh yeah colin smash girls mkr  949380854   \n",
       "1  3.210000e+17      insane keep bringing people back show end mkr  297877558   \n",
       "2  3.240000e+17  berkeley eagle mkr shit show comebacks johnny ...  272704749   \n",
       "3  3.260000e+17                                  sigh oh colin mkr   71416635   \n",
       "4  3.820000e+17  rt brian day swear sexist honestly cannot stan...  930620467   \n",
       "\n",
       "      Screen Name   Class  Label  \n",
       "0    CreatrixKali  sexism      0  \n",
       "1   quincepoacher    none      2  \n",
       "2      MarkTramby    none      2  \n",
       "3          BinnyD    none      2  \n",
       "4  YesYoureSexist  sexism      0  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(df)) < 0.8\n",
    "train, test = df[mask], df[~mask]\n",
    "train.to_csv('./tweets_train', sep='\\t', index=False, header=False)\n",
    "test.to_csv('./tweets_test', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=pd.read_csv(\"./tfidf.csv\", encoding = \"ISO-8859-1\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_t = list(tf['Sexist_terms'])\n",
    "r_t = list(tf['Racist_terms']) \n",
    "o_t = list(tf['Other_terms'])\n",
    "classes = [s_t,r_t,o_t]\n",
    "\n",
    "bad_list = ['colin','im','dont','ok','u','af','na','ur','p','^.^','lt','am','|']\n",
    "\n",
    "for i in classes:\n",
    "    for k in i:\n",
    "        if k in bad_list:\n",
    "            i.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LL ['sorri', 'honestli', 'femin', 'ref', 'footbal', 'kitchen', 'cunt', 'horribl', 'dude', 'driver', 'sport', 'sportscent', 'aw', 'chick', 'espn', 'annoy', 'ladi', 'comedian', 'coach', 'dish', 'score', 'blond', 'rugbi', 'standard', 'omg', 'analyst', 'swear', \"y'all\", 'hoe', 'af', 'shave', 'hair', 'weird', 'ghost', 'mom', 'idk', 'rap', 'wow', 'basketbal', 'slut', 'wrestl', 'fem', 'tit', 'wnba', 'gender', 'ufc', 'charact', 'sister', 'though', 'crazi', 'fan', 'team', 'gon', 'argu', 'wonder', 'smile', 'andr', 'rapper', 'cant', 'hockey', 'eye', 'leagu', 'tonight', 'miss', 'tan', 'thor', 'skank', 'ghostbust', 'bimbo', 'parenthood', 'abort', 'gap', 'romney', 'movement', 'adopt', 'ford', 'valenti', 'kati', 'nikki', 'hooker', 'sausag', 'trashi', 'sassi', 'tart', 'deconstruct', 'promo', 'anni', 'kat', 'strateg', 'lloyd', 'celin']\n",
      "I sorri\n",
      "I honestli\n",
      "I femin\n",
      "I ref\n",
      "I footbal\n",
      "I kitchen\n",
      "I cunt\n",
      "I horribl\n",
      "I dude\n",
      "I driver\n",
      "I sport\n",
      "I sportscent\n",
      "I aw\n",
      "I chick\n",
      "I espn\n",
      "I annoy\n",
      "I ladi\n",
      "I comedian\n",
      "I coach\n",
      "I dish\n",
      "I score\n",
      "I blond\n",
      "I rugbi\n",
      "I standard\n",
      "I omg\n",
      "I analyst\n",
      "I swear\n",
      "I y'all\n",
      "I hoe\n",
      "I af\n",
      "I shave\n",
      "I hair\n",
      "I weird\n",
      "I ghost\n",
      "I mom\n",
      "I idk\n",
      "I rap\n",
      "I wow\n",
      "I basketbal\n",
      "I slut\n",
      "I wrestl\n",
      "I fem\n",
      "I tit\n",
      "I wnba\n",
      "I gender\n",
      "I ufc\n",
      "I charact\n",
      "I sister\n",
      "I though\n",
      "I crazi\n",
      "I fan\n",
      "I team\n",
      "I gon\n",
      "I argu\n",
      "I wonder\n",
      "I smile\n",
      "I andr\n",
      "I rapper\n",
      "I cant\n",
      "I hockey\n",
      "I eye\n",
      "I leagu\n",
      "I tonight\n",
      "I miss\n",
      "I tan\n",
      "I thor\n",
      "I skank\n",
      "I ghostbust\n",
      "I bimbo\n",
      "I parenthood\n",
      "I abort\n",
      "I gap\n",
      "I romney\n",
      "I movement\n",
      "I adopt\n",
      "I ford\n",
      "I valenti\n",
      "I kati\n",
      "I nikki\n",
      "I hooker\n",
      "I sausag\n",
      "I trashi\n",
      "I sassi\n",
      "I tart\n",
      "I deconstruct\n",
      "I promo\n",
      "I anni\n",
      "I kat\n",
      "I strateg\n",
      "I lloyd\n",
      "I celin\n",
      "LL ['testimoni', 'terrorist', 'islamofascist', 'inhuman', 'religion', 'outlaw', 'prophet', 'pedophil', 'caravan', 'robber', 'slave', 'trader', 'scum', 'israel', 'extermin', 'allah', 'fanat', 'islamophob', 'reform', 'convert', 'jihad', 'moham', 'mosul', 'peac', 'slaveri', 'imperi', 'tyrant', 'pervert', 'sharia', 'phoni', 'religi', 'isra', 'microbrain', 'slaughter', 'conquer', 'crusad', 'saudi', 'europ', 'arabian', 'mankind', 'khybar', 'hindu', 'pakistan', 'buddhist', 'contribut', 'freedom', 'ezidi', 'nazi', 'subject', 'invad', 'politician', 'cornerston', 'expedit', 'behead', 'enslav', 'mainstream', 'filth', 'citizen', 'spain', 'islamist', 'arab', 'divis', 'taliban', 'properti', 'islamolunat', 'palestinian', 'hama', 'peninsula', 'jewish', 'imperialist', 'shia', 'non-muslim', 'elect', 'daesh', 'medina', 'koban', 'discoveri', 'persian', 'thug', 'madrassa', 'egotist', 'tribe', 'third', 'pedophelia', 'daeshbag', 'militia', 'slaver', 'occupi', 'lao', 'islamophobia', 'naziphobia', 'graduat', 'mujahedeen', 'palestin', 'ottoman', 'oppon', 'roof', 'mob', 'bangladesh']\n",
      "I testimoni\n",
      "I terrorist\n",
      "I islamofascist\n",
      "I inhuman\n",
      "I religion\n",
      "I outlaw\n",
      "I prophet\n",
      "I pedophil\n",
      "I caravan\n",
      "I robber\n",
      "I slave\n",
      "I trader\n",
      "I scum\n",
      "I israel\n",
      "I extermin\n",
      "I allah\n",
      "I fanat\n",
      "I islamophob\n",
      "I reform\n",
      "I convert\n",
      "I jihad\n",
      "I moham\n",
      "I mosul\n",
      "I peac\n",
      "I slaveri\n",
      "I imperi\n",
      "I tyrant\n",
      "I pervert\n",
      "I sharia\n",
      "I phoni\n",
      "I religi\n",
      "I isra\n",
      "I microbrain\n",
      "I slaughter\n",
      "I conquer\n",
      "I crusad\n",
      "I saudi\n",
      "I europ\n",
      "I arabian\n",
      "I mankind\n",
      "I khybar\n",
      "I hindu\n",
      "I pakistan\n",
      "I buddhist\n",
      "I contribut\n",
      "I freedom\n",
      "I ezidi\n",
      "I nazi\n",
      "I subject\n",
      "I invad\n",
      "I politician\n",
      "I cornerston\n",
      "I expedit\n",
      "I behead\n",
      "I enslav\n",
      "I mainstream\n",
      "I filth\n",
      "I citizen\n",
      "I spain\n",
      "I islamist\n",
      "I arab\n",
      "I divis\n",
      "I taliban\n",
      "I properti\n",
      "I islamolunat\n",
      "I palestinian\n",
      "I hama\n",
      "I peninsula\n",
      "I jewish\n",
      "I imperialist\n",
      "I shia\n",
      "I non-muslim\n",
      "I elect\n",
      "I daesh\n",
      "I medina\n",
      "I koban\n",
      "I discoveri\n",
      "I persian\n",
      "I thug\n",
      "I madrassa\n",
      "I egotist\n",
      "I tribe\n",
      "I third\n",
      "I pedophelia\n",
      "I daeshbag\n",
      "I militia\n",
      "I slaver\n",
      "I occupi\n",
      "I lao\n",
      "I islamophobia\n",
      "I naziphobia\n",
      "I graduat\n",
      "I mujahedeen\n",
      "I palestin\n",
      "I ottoman\n",
      "I oppon\n",
      "I roof\n",
      "I mob\n",
      "I bangladesh\n",
      "LL ['sorri', 'eat', 'kitchen', 'instant', 'dude', 'minut', 'dish', 'score', 'omg', 'weird', 'wow', 'though', 'team', 'round', 'andr', 'judg', 'uh', 'tonight', 'mention', 'miss', 'contest', 'awesom', 'gamerg', 'gg', 'plate', 'kati', 'nikki', 'harass', 'tech', 'tart', 'dessert', 'deconstruct', 'lemon', 'anni', 'ash', 'camilla', 'sheri', 'kat', 'elimin', 'pete', 'strateg', 'lloyd', 'manu', 'emili', 'sudden', 'terrorist', 'religion', 'moham', 'mosul', 'microbrain', 'daesh', 'koban', 'code', 'shitti', 'yay', 'puppi', 'random', 'bulli', 'coffe', 'email', 'russia', 'user', 'chicken', 'updat', 'ypg/ypj', 'pesh', 'coalit', 'facebook', 'apart', 'wadhwa', 'tomorrow', 'freebsd', 'github', 'yup', 'ethic', 'gater', 'perl', 'dev', 'leo', 'upset', 'gdc', 'appl', 'edit', 'bacon', 'oapi', 'liver', 'jac', 'shaz', 'yummi', 'mapl']\n",
      "I sorri\n",
      "I eat\n",
      "I kitchen\n",
      "I instant\n",
      "I dude\n",
      "I minut\n",
      "I dish\n",
      "I score\n",
      "I omg\n",
      "I weird\n",
      "I wow\n",
      "I though\n",
      "I team\n",
      "I round\n",
      "I andr\n",
      "I judg\n",
      "I uh\n",
      "I tonight\n",
      "I mention\n",
      "I miss\n",
      "I contest\n",
      "I awesom\n",
      "I gamerg\n",
      "I gg\n",
      "I plate\n",
      "I kati\n",
      "I nikki\n",
      "I harass\n",
      "I tech\n",
      "I tart\n",
      "I dessert\n",
      "I deconstruct\n",
      "I lemon\n",
      "I anni\n",
      "I ash\n",
      "I camilla\n",
      "I sheri\n",
      "I kat\n",
      "I elimin\n",
      "I pete\n",
      "I strateg\n",
      "I lloyd\n",
      "I manu\n",
      "I emili\n",
      "I sudden\n",
      "I terrorist\n",
      "I religion\n",
      "I moham\n",
      "I mosul\n",
      "I microbrain\n",
      "I daesh\n",
      "I koban\n",
      "I code\n",
      "I shitti\n",
      "I yay\n",
      "I puppi\n",
      "I random\n",
      "I bulli\n",
      "I coffe\n",
      "I email\n",
      "I russia\n",
      "I user\n",
      "I chicken\n",
      "I updat\n",
      "I ypg/ypj\n",
      "I pesh\n",
      "I coalit\n",
      "I facebook\n",
      "I apart\n",
      "I wadhwa\n",
      "I tomorrow\n",
      "I freebsd\n",
      "I github\n",
      "I yup\n",
      "I ethic\n",
      "I gater\n",
      "I perl\n",
      "I dev\n",
      "I leo\n",
      "I upset\n",
      "I gdc\n",
      "I appl\n",
      "I edit\n",
      "I bacon\n",
      "I oapi\n",
      "I liver\n",
      "I jac\n",
      "I shaz\n",
      "I yummi\n",
      "I mapl\n"
     ]
    }
   ],
   "source": [
    "tweets = list(df['Tweets']) \n",
    "\n",
    "for l in classes:\n",
    "    print (\"LL\",l)\n",
    "    for i in l:\n",
    "        print (\"I\",i)\n",
    "        t = []\n",
    "        for d in tweets:\n",
    "            tfm = re.search(i, d)\n",
    "            if tfm:\n",
    "                t.append(1)\n",
    "            else:\n",
    "                t.append(0)\n",
    "        df[i] = t    \n",
    "\n",
    "    \n",
    "names = df.columns.values \n",
    "#print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User Id</th>\n",
       "      <th>Screen Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>Label</th>\n",
       "      <th>sorri</th>\n",
       "      <th>honestli</th>\n",
       "      <th>femin</th>\n",
       "      <th>ref</th>\n",
       "      <th>...</th>\n",
       "      <th>gdc</th>\n",
       "      <th>appl</th>\n",
       "      <th>edit</th>\n",
       "      <th>bacon</th>\n",
       "      <th>oapi</th>\n",
       "      <th>liver</th>\n",
       "      <th>jac</th>\n",
       "      <th>shaz</th>\n",
       "      <th>yummi</th>\n",
       "      <th>mapl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.190000e+17</td>\n",
       "      <td>oh yeah colin smash girls mkr</td>\n",
       "      <td>949380854</td>\n",
       "      <td>CreatrixKali</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.210000e+17</td>\n",
       "      <td>insane keep bringing people back show end mkr</td>\n",
       "      <td>297877558</td>\n",
       "      <td>quincepoacher</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.240000e+17</td>\n",
       "      <td>berkeley eagle mkr shit show comebacks johnny ...</td>\n",
       "      <td>272704749</td>\n",
       "      <td>MarkTramby</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.260000e+17</td>\n",
       "      <td>sigh oh colin mkr</td>\n",
       "      <td>71416635</td>\n",
       "      <td>BinnyD</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.820000e+17</td>\n",
       "      <td>rt brian day swear sexist honestly cannot stan...</td>\n",
       "      <td>930620467</td>\n",
       "      <td>YesYoureSexist</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Tweets    User Id  \\\n",
       "0  3.190000e+17                      oh yeah colin smash girls mkr  949380854   \n",
       "1  3.210000e+17      insane keep bringing people back show end mkr  297877558   \n",
       "2  3.240000e+17  berkeley eagle mkr shit show comebacks johnny ...  272704749   \n",
       "3  3.260000e+17                                  sigh oh colin mkr   71416635   \n",
       "4  3.820000e+17  rt brian day swear sexist honestly cannot stan...  930620467   \n",
       "\n",
       "      Screen Name   Class  Label  sorri  honestli  femin  ref  ...   gdc  \\\n",
       "0    CreatrixKali  sexism      0      0         0      0    0  ...     0   \n",
       "1   quincepoacher    none      2      0         0      0    0  ...     0   \n",
       "2      MarkTramby    none      2      0         0      0    0  ...     0   \n",
       "3          BinnyD    none      2      0         0      0    0  ...     0   \n",
       "4  YesYoureSexist  sexism      0      0         0      0    0  ...     0   \n",
       "\n",
       "   appl  edit  bacon  oapi  liver  jac  shaz  yummi  mapl  \n",
       "0     0     0      0     0      0    0     0      0     0  \n",
       "1     0     0      0     0      0    0     0      0     0  \n",
       "2     0     0      0     0      0    0     0      0     0  \n",
       "3     0     0      0     0      0    0     0      0     0  \n",
       "4     0     0      0     0      0    0     0      0     0  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(df.Tweets)\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "negative = []\n",
    "compound = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    vs = analyzer.polarity_scores(tweets[i])\n",
    "    negative.append(vs.get(\"neg\"))\n",
    "    compound.append(vs.get(\"compound\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Neg_Sent'] = negative\n",
    "df['Compound'] = compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"final_dataset_hs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User Id</th>\n",
       "      <th>Screen Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>Label</th>\n",
       "      <th>sorri</th>\n",
       "      <th>honestli</th>\n",
       "      <th>femin</th>\n",
       "      <th>ref</th>\n",
       "      <th>...</th>\n",
       "      <th>gdc</th>\n",
       "      <th>appl</th>\n",
       "      <th>edit</th>\n",
       "      <th>bacon</th>\n",
       "      <th>oapi</th>\n",
       "      <th>liver</th>\n",
       "      <th>jac</th>\n",
       "      <th>shaz</th>\n",
       "      <th>yummi</th>\n",
       "      <th>mapl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.190000e+17</td>\n",
       "      <td>oh yeah colin smash girls mkr</td>\n",
       "      <td>949380854</td>\n",
       "      <td>CreatrixKali</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.210000e+17</td>\n",
       "      <td>insane keep bringing people back show end mkr</td>\n",
       "      <td>297877558</td>\n",
       "      <td>quincepoacher</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.240000e+17</td>\n",
       "      <td>berkeley eagle mkr shit show comebacks johnny ...</td>\n",
       "      <td>272704749</td>\n",
       "      <td>MarkTramby</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.260000e+17</td>\n",
       "      <td>sigh oh colin mkr</td>\n",
       "      <td>71416635</td>\n",
       "      <td>BinnyD</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.820000e+17</td>\n",
       "      <td>rt brian day swear sexist honestly cannot stan...</td>\n",
       "      <td>930620467</td>\n",
       "      <td>YesYoureSexist</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Tweets    User Id  \\\n",
       "0  3.190000e+17                      oh yeah colin smash girls mkr  949380854   \n",
       "1  3.210000e+17      insane keep bringing people back show end mkr  297877558   \n",
       "2  3.240000e+17  berkeley eagle mkr shit show comebacks johnny ...  272704749   \n",
       "3  3.260000e+17                                  sigh oh colin mkr   71416635   \n",
       "4  3.820000e+17  rt brian day swear sexist honestly cannot stan...  930620467   \n",
       "\n",
       "      Screen Name   Class  Label  sorri  honestli  femin  ref  ...   gdc  \\\n",
       "0    CreatrixKali  sexism      0      0         0      0    0  ...     0   \n",
       "1   quincepoacher    none      2      0         0      0    0  ...     0   \n",
       "2      MarkTramby    none      2      0         0      0    0  ...     0   \n",
       "3          BinnyD    none      2      0         0      0    0  ...     0   \n",
       "4  YesYoureSexist  sexism      0      0         0      0    0  ...     0   \n",
       "\n",
       "   appl  edit  bacon  oapi  liver  jac  shaz  yummi  mapl  \n",
       "0     0     0      0     0      0    0     0      0     0  \n",
       "1     0     0      0     0      0    0     0      0     0  \n",
       "2     0     0      0     0      0    0     0      0     0  \n",
       "3     0     0      0     0      0    0     0      0     0  \n",
       "4     0     0      0     0      0    0     0      0     0  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test sets\n",
    "Columns=['Tweets']\n",
    "x=df[Columns]\n",
    "y=df['Label']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./final_dataset_hs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9060, 2)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-b34e08fad685>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"word\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_data_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain_data_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(analyzer = \"word\",tokenizer = None,preprocessor = None,stop_words = None,max_features = 5000)\n",
    "\n",
    "train_data_features=vectorizer.fit_transform(X_train)\n",
    "train_data_features=train_data_features.toarray()\n",
    "\n",
    "test_data_features=vectorizer.transform(X_test)\n",
    "test_data_features=test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 9060]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-f255cd26ad78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 9060]"
     ]
    }
   ],
   "source": [
    "#SVM with linear kernel\n",
    "clf=svm.SVC(kernel='linear',C=1.0)\n",
    "print (\"Training\")\n",
    "clf.fit(train_data_features,Y_train)\n",
    "\n",
    "print (\"Testing\")\n",
    "predicted=clf.predict(test_data_features)\n",
    "accuracy=np.mean(predicted==Y_test)\n",
    "print (\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8678745028722934, 0.8678745028722934, 0.8678745028722934, None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(Y_test, predicted, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./final_dataset_hs.csv\")\n",
    "#tf=pd.read_csv(\"./tfidf.csv\", encoding = \"ISO-8859-1\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_t = list(tf['Sexist_terms'])\n",
    "r_t = list(tf['Racist_terms']) \n",
    "o_t = list(tf['Other_terms'])\n",
    "classes = [s_t,r_t,o_t]\n",
    "\n",
    "bad_list = ['colin','im','dont','ok','u','af','na','ur','p','^.^','lt','am','|']\n",
    "\n",
    "for i in classes:\n",
    "    for k in i:\n",
    "        if k in bad_list:\n",
    "            i.remove(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(df['Tweets']) \n",
    "\n",
    "for l in classes:\n",
    "    for i in l:\n",
    "        t = []\n",
    "        for d in tweets:\n",
    "            tfm = re.search(i, d)\n",
    "            if tfm:\n",
    "                t.append(1)\n",
    "            else:\n",
    "                t.append(0)\n",
    "        df[i] = t    \n",
    "\n",
    "    \n",
    "names = df.columns.values \n",
    "#print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = list(df['Hashtags'])\n",
    "htags = []\n",
    "\n",
    "for hashtag in hashtags:\n",
    "    texts = re.findall(r\"'\\w+'\", hashtag)\n",
    "    for i in texts:\n",
    "        if i != 'text' and i != 'indices':\n",
    "            htags.append(i.lower())\n",
    "\n",
    "htags=set(htags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_h = list(df['Hashtags'])\n",
    "\n",
    "for i in htags:\n",
    "    list_i = []\n",
    "    for k in list_h:\n",
    "        ht = re.search(i,k)\n",
    "        if ht:\n",
    "            list_i.append(1)\n",
    "        else:\n",
    "            list_i.append(0)\n",
    "    df[i] = list_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "tweets = list(df.Tweets)\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "negative = []\n",
    "compound = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    vs = analyzer.polarity_scores(tweets[i])\n",
    "    negative.append(vs.get(\"neg\"))\n",
    "    compound.append(vs.get(\"compound\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Neg_Sent'] = negative\n",
    "df['Compound'] = compound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User Id</th>\n",
       "      <th>Screen Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>Label</th>\n",
       "      <th>sorri</th>\n",
       "      <th>honestli</th>\n",
       "      <th>femin</th>\n",
       "      <th>ref</th>\n",
       "      <th>...</th>\n",
       "      <th>edit</th>\n",
       "      <th>bacon</th>\n",
       "      <th>oapi</th>\n",
       "      <th>liver</th>\n",
       "      <th>jac</th>\n",
       "      <th>shaz</th>\n",
       "      <th>yummi</th>\n",
       "      <th>mapl</th>\n",
       "      <th>Neg_Sent</th>\n",
       "      <th>Compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.190000e+17</td>\n",
       "      <td>oh yeah colin smash girls mkr</td>\n",
       "      <td>949380854</td>\n",
       "      <td>CreatrixKali</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.210000e+17</td>\n",
       "      <td>insane keep bringing people back show end mkr</td>\n",
       "      <td>297877558</td>\n",
       "      <td>quincepoacher</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.240000e+17</td>\n",
       "      <td>berkeley eagle mkr shit show comebacks johnny ...</td>\n",
       "      <td>272704749</td>\n",
       "      <td>MarkTramby</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.260000e+17</td>\n",
       "      <td>sigh oh colin mkr</td>\n",
       "      <td>71416635</td>\n",
       "      <td>BinnyD</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.820000e+17</td>\n",
       "      <td>rt brian day swear sexist honestly cannot stan...</td>\n",
       "      <td>930620467</td>\n",
       "      <td>YesYoureSexist</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Tweets    User Id  \\\n",
       "0  3.190000e+17                      oh yeah colin smash girls mkr  949380854   \n",
       "1  3.210000e+17      insane keep bringing people back show end mkr  297877558   \n",
       "2  3.240000e+17  berkeley eagle mkr shit show comebacks johnny ...  272704749   \n",
       "3  3.260000e+17                                  sigh oh colin mkr   71416635   \n",
       "4  3.820000e+17  rt brian day swear sexist honestly cannot stan...  930620467   \n",
       "\n",
       "      Screen Name   Class  Label  sorri  honestli  femin  ref    ...     edit  \\\n",
       "0    CreatrixKali  sexism      0      0         0      0    0    ...        0   \n",
       "1   quincepoacher    none      2      0         0      0    0    ...        0   \n",
       "2      MarkTramby    none      2      0         0      0    0    ...        0   \n",
       "3          BinnyD    none      2      0         0      0    0    ...        0   \n",
       "4  YesYoureSexist  sexism      0      0         0      0    0    ...        0   \n",
       "\n",
       "   bacon  oapi  liver  jac  shaz  yummi  mapl  Neg_Sent  Compound  \n",
       "0      0     0      0    0     0      0     0     0.000    0.2960  \n",
       "1      0     0      0    0     0      0     0     0.278   -0.4019  \n",
       "2      0     0      0    0     0      0     0     0.243   -0.3400  \n",
       "3      0     0      0    0     0      0     0     0.000    0.0258  \n",
       "4      0     0      0    0     0      0     0     0.079    0.4215  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"full_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sorri  honestli  femin  ref  footbal  kitchen  cunt  horribl  dude  driver  \\\n",
      "0      0         0      0    0        0        0     0        0     0       0   \n",
      "1      0         0      0    0        0        0     0        0     0       0   \n",
      "2      0         0      0    0        0        0     0        0     0       0   \n",
      "3      0         0      0    0        0        0     0        0     0       0   \n",
      "4      0         0      0    0        1        0     0        0     0       0   \n",
      "\n",
      "   ...    upset  gdc  appl  edit  bacon  oapi  liver  jac  shaz  yummi  \n",
      "0  ...        0    0     0     0      0     0      0    0     0      0  \n",
      "1  ...        0    0     0     0      0     0      0    0     0      0  \n",
      "2  ...        0    0     0     0      0     0      0    0     0      0  \n",
      "3  ...        0    0     0     0      0     0      0    0     0      0  \n",
      "4  ...        0    0     0     0      0     0      0    0     0      0  \n",
      "\n",
      "[5 rows x 251 columns]\n"
     ]
    }
   ],
   "source": [
    "rel_var = df.iloc[:,6:257]\n",
    "print(rel_var.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shreya Majumder\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "predictors = pd.DataFrame.as_matrix(rel_var, columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "new_class = []\n",
    "\n",
    "for i in df.Class:\n",
    "    if i == 'sexism':\n",
    "        new_class.append(0)\n",
    "    elif i == 'racism':\n",
    "        new_class.append(1)\n",
    "    else:\n",
    "        new_class.append(2)\n",
    "\n",
    "target=to_categorical(new_class)\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "print (n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#Add layers to model\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer bidirectional_6: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-00537241564b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m model.fit(predictors, target,\n\u001b[0;32m      8\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             callbacks=[early_stop])\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#early_stop = EarlyStopping(patience=3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# to match the value shapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[1;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[0;32m    587\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer bidirectional_6: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "early_stop = EarlyStopping(patience=3) \n",
    "model.fit(predictors, target,\n",
    "            epochs=30,\n",
    "            callbacks=[early_stop])\n",
    "\n",
    "#early_stop = EarlyStopping(patience=3) \n",
    "\n",
    "#fit model\n",
    "#model.fit(predictors, target, validation_split=0.3, epochs=30, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-f8556f353e6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./tweets_test.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;31m# might mutate self.engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_file_or_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clean_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'has_index_names'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_clean_options\u001b[1;34m(self, options, engine)\u001b[0m\n\u001b[0;32m    868\u001b[0m                                   \u001b[1;34m\" sep=None with delim_whitespace=False\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                 \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0msep\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mr'\\s+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'delim_whitespace'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "max_features = 50000\n",
    "maxlen = 10\n",
    "batch_size = 32\n",
    "#Create train and test sets\n",
    "#train = './tweets_train.csv'\n",
    "#test = './tweets_test.csv'\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.Tweets, df.Label, test_size=0.2)\n",
    "#x_train, y_train = read_data(train, max_features)\n",
    "#x_test, y_test = read_data(test, max_features)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=30,\n",
    "            validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User Id</th>\n",
       "      <th>Screen Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>Label</th>\n",
       "      <th>sorri</th>\n",
       "      <th>honestli</th>\n",
       "      <th>femin</th>\n",
       "      <th>ref</th>\n",
       "      <th>...</th>\n",
       "      <th>edit</th>\n",
       "      <th>bacon</th>\n",
       "      <th>oapi</th>\n",
       "      <th>liver</th>\n",
       "      <th>jac</th>\n",
       "      <th>shaz</th>\n",
       "      <th>yummi</th>\n",
       "      <th>mapl</th>\n",
       "      <th>Neg_Sent</th>\n",
       "      <th>Compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.190000e+17</td>\n",
       "      <td>oh yeah colin smash girls mkr</td>\n",
       "      <td>949380854</td>\n",
       "      <td>CreatrixKali</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.210000e+17</td>\n",
       "      <td>insane keep bringing people back show end mkr</td>\n",
       "      <td>297877558</td>\n",
       "      <td>quincepoacher</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.240000e+17</td>\n",
       "      <td>berkeley eagle mkr shit show comebacks johnny ...</td>\n",
       "      <td>272704749</td>\n",
       "      <td>MarkTramby</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.260000e+17</td>\n",
       "      <td>sigh oh colin mkr</td>\n",
       "      <td>71416635</td>\n",
       "      <td>BinnyD</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.820000e+17</td>\n",
       "      <td>rt brian day swear sexist honestly cannot stan...</td>\n",
       "      <td>930620467</td>\n",
       "      <td>YesYoureSexist</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Tweets    User Id  \\\n",
       "0  3.190000e+17                      oh yeah colin smash girls mkr  949380854   \n",
       "1  3.210000e+17      insane keep bringing people back show end mkr  297877558   \n",
       "2  3.240000e+17  berkeley eagle mkr shit show comebacks johnny ...  272704749   \n",
       "3  3.260000e+17                                  sigh oh colin mkr   71416635   \n",
       "4  3.820000e+17  rt brian day swear sexist honestly cannot stan...  930620467   \n",
       "\n",
       "      Screen Name   Class  Label  sorri  honestli  femin  ref    ...     edit  \\\n",
       "0    CreatrixKali  sexism      0      0         0      0    0    ...        0   \n",
       "1   quincepoacher    none      2      0         0      0    0    ...        0   \n",
       "2      MarkTramby    none      2      0         0      0    0    ...        0   \n",
       "3          BinnyD    none      2      0         0      0    0    ...        0   \n",
       "4  YesYoureSexist  sexism      0      0         0      0    0    ...        0   \n",
       "\n",
       "   bacon  oapi  liver  jac  shaz  yummi  mapl  Neg_Sent  Compound  \n",
       "0      0     0      0    0     0      0     0     0.000    0.2960  \n",
       "1      0     0      0    0     0      0     0     0.278   -0.4019  \n",
       "2      0     0      0    0     0      0     0     0.243   -0.3400  \n",
       "3      0     0      0    0     0      0     0     0.000    0.0258  \n",
       "4      0     0      0    0     0      0     0     0.079    0.4215  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# NLTK\n",
    "\n",
    "\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import timeit\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample):\n",
    "    \n",
    "    print(\"\\nGiven sample size:\", len(sample))\n",
    "    \n",
    "    #Keras tokenizer function to tokenize the strings and \n",
    "    #‘texts_to_sequences’ to make sequences of words.\n",
    "\n",
    "    vocabulary_size = 20000\n",
    "\n",
    "    #Maximum number of words to work with \n",
    "    #if set, tokenization will be restricted to the top nb_words most common words in the dataset).\n",
    "    tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "\n",
    "    #fit_on_texts(texts):\n",
    "    #Arguments: list of texts to train on.\n",
    "    #tokenizer.fit_on_texts(data['tweet'])\n",
    "    tokenizer.fit_on_texts(sample)\n",
    "\n",
    "    #texts_to_sequences(texts)\n",
    "    #texts: list of texts to turn to sequences.\n",
    "    #Return: list of sequences (one per text input).\n",
    "    \n",
    "    #sequences = tokenizer.texts_to_sequences(data['tweet'])\n",
    "    sequences = tokenizer.texts_to_sequences(sample)\n",
    "    sample = pad_sequences(sequences, maxlen=50)\n",
    "    \n",
    "    print(\"Processed sample shape:\", sample.shape)\n",
    "    #print(\"Sample1:\", sample[0])\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len: 9060 \n",
      "Test data len: 2265\n",
      "\n",
      "Given sample size: 9060\n",
      "Processed sample shape: (9060, 50)\n",
      "\n",
      "Given sample size: 2265\n",
      "Processed sample shape: (2265, 50)\n"
     ]
    }
   ],
   "source": [
    "X = df['Tweets']\n",
    "Y = df['Label']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "print(\"Train data len:\", len(X_train), \"\\nTest data len:\", len(X_test))\n",
    "\n",
    "X_train_seq = process_sample(X_train)\n",
    "X_test_seq = process_sample(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute '_feed_input_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-2a1224bc762f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[1;31m# Case: symbolic-mode subclassed network.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[1;31m# Do not do shape validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[0mfeed_input_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_feed_input_names'"
     ]
    }
   ],
   "source": [
    "#LSTM Network architecture\n",
    "\n",
    "print('Building model...')\n",
    "model_lstm = Sequential()\n",
    "\n",
    "#The network starts with an embedding layer.\n",
    "#Turns positive integers (indexes) into dense vectors of fixed size allowing the n/w to represent a word in a meaningful way.\n",
    "#eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "#This layer can only be used as the first layer in a model.\n",
    "\n",
    "#keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', ***, input_length=None)\n",
    "\n",
    "#input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "\n",
    "#output_dim: int >= 0. Dimension of the dense embedding.\n",
    "\n",
    "#input_length: Length of input sequences, when it is constant. \n",
    "#This argument is required if you are going to connect Flatten then Dense layers upstream \n",
    "#(without it, the shape of the dense outputs cannot be computed).\n",
    "\n",
    "#eg. model.add(Embedding(1000, 64, input_length=10))\n",
    "\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# where the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "\n",
    "# o/p will be model.output_shape == (None, 10 :input_dim, 64:output_dim), where None is the batch dimension of the matrix given.\n",
    "\n",
    "\n",
    "model_lstm.add(Embedding(20000, 100, input_length=251))\n",
    "model_lstm.add(LSTM(100, dropout=0.5))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(patience=2) \n",
    "\n",
    "#fit model\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=20, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7927 samples, validate on 3398 samples\n",
      "Epoch 1/100\n",
      "7927/7927 [==============================] - 48s 6ms/step - loss: 0.6754 - acc: 0.6754 - val_loss: 0.4724 - val_acc: 0.8514\n",
      "Epoch 2/100\n",
      "7927/7927 [==============================] - 46s 6ms/step - loss: 0.6444 - acc: 0.6840 - val_loss: 0.4893 - val_acc: 0.8514\n",
      "Epoch 3/100\n",
      "7927/7927 [==============================] - 47s 6ms/step - loss: 0.6384 - acc: 0.6841 - val_loss: 0.4716 - val_acc: 0.8514\n",
      "Epoch 4/100\n",
      "7927/7927 [==============================] - 47s 6ms/step - loss: 0.6412 - acc: 0.6841 - val_loss: 0.5076 - val_acc: 0.8514\n",
      "Epoch 5/100\n",
      "7927/7927 [==============================] - 48s 6ms/step - loss: 0.6400 - acc: 0.6841 - val_loss: 0.5052 - val_acc: 0.8514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f03d849940>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#Add layers to model\n",
    "model.add(Embedding(20000, 100, input_length=n_cols))\n",
    "model.add(Dropout(0.25))#, input_shape=(sequence_length, embedding_dim)))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "#compile \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(patience=2) \n",
    "\n",
    "#fit model\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=100, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
